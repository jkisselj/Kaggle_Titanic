# Проект Kaggle: "Титаник"

## Username
* **Kaggle Username:** [jegorkisseljv]
* **Школьный Username:** [jkisselj01EDU tallinn_11_2025] (указан в `username.txt`)

## Feature Engineering (Созданные признаки)
Для улучшения базовой модели были созданы следующие признаки:

* **Title (Титул):** Извлечен из `Name` (Mr, Mrs, Miss, Master, Rare).
* **FamilySize (Размер семьи):** `SibSp` + `Parch` + 1.
* **IsAlone (Одиночка):** Бинарный признак (1, если `FamilySize` == 1).
* **HasCabin (Есть каюта):** Бинарный признак (1, если `Cabin` не NaN).

## Модели
1.  **LogisticRegression (Базовая):** ~76%
2.  **RandomForestClassifier (V8, V12):** Достигнут результат **78.5%** с настройкой `max_depth=5`.
3.  **GradientBoostingClassifier (V13):**

# Описание
Цель этого проекта — построить модель машинного обучения, которая предсказывает выживание пассажиров Титаника. Задание требует достичь минимальной точности 78.9% на приватном лидерборде Kaggle.


1. Обзор проекта

Этот проект — мое решение классической задачи Kaggle "Titanic - Machine Learning from Disaster".

Цель: Построить модель машинного обучения, которая предсказывает выживание пассажиров, основываясь на предоставленных данных (train.csv).

Требование для аудита: Достичь минимальной точности 78.9% на приватном лидерборде Kaggle.

2. Финальный результат

 Лучший результат на Kaggle: 78.95% (проходной порог 78.90% преодолен).

3. Структура проекта

Проект организован в соответствии с требованиями аудита:

project/
│
├── data/                 # Cырые данные (train.csv, test.csv)
│   ├── train.csv
│   ├── test.csv
│   └── gender_submission.csv
│
├── notebook/
│   ├── main.ipynb        # Финальный "чистый" код, дающий 78.95%
│   └── EDA.ipynb         # (Опционально) Архивный ноутбук с моими
│                         # исследованиями LogReg и графиками весов.
│
├── environment.yml       # Файл окружения Conda для 100% воспроизводимости
├── README.md             # Этот файл
└── username.txt          # Мой школьный username


# 4. Мой путь к 78.9% (Хронология и выводы)

 Мой путь состоял из 4-х основных этапов, включая "провальные" эксперименты, которые помогли мне найти "золотую середину".

# Этап 1: "Наивный" подход (Логистическая Регрессия)

Я начал с самой простой (линейной) модели — LogisticRegression. Я заведомо знал, что она не преодолеет высокий порог, но мне было интересно, как далеко можно "продвинуть" примитивную модель, если "накормить" ее "умными" фичами.

# Фичи:  Title, FamilySize, IsAlone, Deck (из Cabin).

* ** Проблема 1  (Кодирование): Я понял, что .map({'Mr': 1, 'Miss': 2}) — это "ловушка" (Label Encoding), которая "ломает" линейную модель, создавая фальшивый порядок. Я исправил это, применив One-Hot Encoding (pd.get_dummies).

Проблема 2 (Масштаб):  фича Fare (цена от 0 до 512) "забивает" Pclass (1-3), исправил это, применив StandardScaler.

Результат: ~76.9%. Модель стала умнее, но все равно не могла уловить сложные взаимодействия фичей (например, "женщина 3-го класса" — это не то же самое, что "женщина 1-го класса").


# Этап 2: Поиск "Чемпиона" (Random Forest)

Я перешел к нелинейной модели — RandomForestClassifier. Она "думает" деревьями решений (if-then-else) и обожает взаимодействия.

Код V8 (первый Чемпион): Используя простой набор фичей (без One-Hot Deck, но с HasCabin) и "безопасные" настройки (max_depth=5, min_samples_leaf=3), я получил свой первый стабильный высокий результат: 78.5%.

# Этап 3: "Провальный" путь (Переобучение)

Достигнув 78.5%, я попытался пробить потолок, пробуя более крутые модели и фичи, но это привело только к переобучению (Overfitting).

GradientBoosting (Антирекорд 76.5%): Я попробовал GradientBoostingClassifier. Модель оказалась слишком "агрессивной" и чувствительной. Она вызубрила train.csv и провалилась на test.csv, дав результат хуже, чем даже примитивная LogisticRegression.

RandomForest (Глубокий): Я попробовал углубить модел (max_depth=7). Результат упал до 77.0%.

GridSearchCV (Агрессивный тюнинг): Я запустил GridSearchCV по 108 комбинациям. Он показал на Kaggle 77.9%.

Вывод: Все эти "провалы" доказали мне: Простота > Переусложнение. Мой "чемпион" (max_depth=5) был близок к идеалу, а все "улучшения" были на самом деле "шумом".

# Этап 4: Финальная "Чемпионская" модель (78.95%)

Я вернулся к своему "чемпиону" (RandomForest с max_depth=5) и улучшил не модель, а ДАННЫЕ — используя инсайты из референс-проектов.

Я оставил код чистым и минималистичным, но добавил 3 "умных" штриха:

Единая обработка: Объединил train и test до обработки, чтобы гарантировать 100% одинаковую подготовку.

Умное заполнение Age: Вместо заполнения средним по больнице (28 лет), я заполнил пропуски Age медианой, сгруппированной по Pclass + Sex. (Пропущенный возраст 5-летнего мальчика в 3-м классе заполнился 6-ю годами, а не 28-ю).

Умная фича FarePerPerson: Я создал фичу Fare / FamilySize. Это гораздо "честнее", чем общая цена билета.

Эти три умных улучшения, примененные к моей "безопасной" модели (max_depth=5), и дали финальный результат.

#  5. Финальный Feature Engineering

"Чемпионский" код в main.ipynb использует только этот минималистичный набор фичей:

Pclass: (Как есть).

Age: Пропуски заполнены медианой по Pclass + Sex.

Fare: Пропуски заполнены медианой.

Sex: Закодировано через .map() (male: 0, female: 1).

Embarked: Пропуски заполнены модой; закодировано через pd.get_dummies().

Title: Извлечено из Name, очищено до 5 групп (Mr, Miss, Mrs, Master, Rare), закодировано через .map().

FamilySize: SibSp + Parch + 1.

IsAlone: (1, если FamilySize == 1).

FarePerPerson: Fare / FamilySize.

(От чего отказался): Deck, TicketPrefix, Cabin и сложные взаимодействия (Age*Class) — все они добавляли шум и ухудшали результат на Kaggle, приводя к переобучению.

# 6. Финальная Модель (Как в main.ipynb)

Модель: RandomForestClassifier

Ключевые параметры (Гиперпараметры):

n_estimators=700 (больше деревьев для стабильности)

max_depth=5 (наша "золотая середина" против переобучения)

min_samples_leaf=3 ("безопасный" параметр, запрещающий модели "зубрить")

random_state=1 (для 100% воспроизводимости)

# 7. Выводы

Простота > Переусложнение: Мой лучший результат (78.95%) был достигнут не сложными моделями (GradientBoosting) или  тюнингом (GridSearchCV), а чистым набором фичей и безопасной настройкой RandomForest.

Feature Engineering — это ключ: Умное заполнение Age и создание FarePerPerson дали те 0.4%, которых не хватало.

RandomForest vs. Boosting: Для этого набора данных RandomForest оказался более стабильным и менее склонным к переобучению, чем GradientBoosting.